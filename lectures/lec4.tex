\label{lec:mac}

So far, we have talked about authenticating \emph{people} and authenticating \emph{files}. In this section, we will discuss authenticating \emph{communication}. If we have two parties that are communicating over the network, we want some way to guarantee to each party that the message they received really came from the other party and was not tampered with along the way.

At a first glance, this seems impossible. If there is some eavesdropper Eve in between the two parties, they can just replace the message with one of their own choosing and the other party will have no idea. To make this possible, we need to relax the scenario a bit and add an assumption---that the two parties share some secret key $k$. 

With this shared key $k$ between the two parties, our goal will be to add some \textquote{tag} onto the message that validates its authenticity. Necessarily, this tag will be a function of this shared key $k$. If this were not the case, the eavesdropper would be able to compute a valid tag herself---the secret $k$ is the only information in this scenario that Eve does not know.

% figure out how to use cryptocode and make a diagram like

%    k              k 
% client -------> server
%         m, tag
%\procedureblock

\section{Defining message authentication codes}

\paragraph{Syntax.}
A message authentication code (MAC) 
over key space $\mathcal{K}$, 
message space $\mathcal{M}$ , and tag space $\calT$
is an efficient algorithm 
$\MAC \colon \calK \times \calM \to \calT$.
In order for a MAC to be useful, it must be \emph{secure},
in the following sense.
We first give the definition and then explain why it is a useful
one:

\begin{definition}[MAC Security: Existentially unforgeability against adaptive chosen message attacks]\label{def:mac-sec}
A MAC $\MAC$ over key space $\calK$ and message space $\mathcal{M}$ is secure 
(existentially unforgeable against adaptive chosen
message attacks) if any poly-time adversary
$\mathcal{A}$\marginnote{In practice,
\textquote{a poly-time adversary} means
\textquote{any real-life adversary}. But we need
to place some mathematical bound on real-life to
make the proofs work out.} wins the following
game with at most negligible probability:
  \begin{itemize}[noitemsep]
    \item The challenger samples a MAC key $k \getsr \calK$.
    \item For $i = 1, 2, \dots$ (polynomially many times)
      \begin{itemize}
        \item The adversary sends any message $m_i \in \calM$ 
              to the challenger 
        \item The challenger responds with $\mathsf{MAC}(k, m_i)$. 
      \end{itemize}
    \item The adversary sends the challenger a message-tag pair $(m^*, t^*)$.
    \item The adversary wins the game if $\mathsf{MAC}(k, m^*) = t^*$ and $m^* \notin \set{m_1, m_2, \ldots, m_n}$.
  \end{itemize}
  \marginnote{A subtlety of this definition is that, even if the MAC scheme is
  secure under this definition, it is possible for an adversary, given 
  a valid message-tag pair $(m,t)$ to produce a second valid message-tag pair
  $(m,t')$ on the same message without knowing the secret key.
  }

	% TODO: finish this diagram
	%\procedureblock{MAC Security}{
	%	$\mathcal{A}$ \> \> \textbf{Ch} \\
	%}
\end{definition}

\subsection{Intuition for the security definition}
To formulate our security notion, we need to define
the adversary's goal and the adversary's power.

The adversary's goal in this definition is 
to compute a valid MAC of \emph{any} message $m\in \mathcal{M}$
of its choice.
It's not entirely obvious why we care about the adversary producing 
a valid MAC on \textit{any} message: ``If the
adversary MACs a message that is jibberish, they
are unlikely to be able to do any harm with it,''
you might think. 
But there will certainly be applications
that authenticate messages that violate whatever
definition of \textquote{non-jibberish} we define.
So allowing the adversary to forge a MAC tag on any 
message makes the definition as broadly applicable as possible.

\marginnote{This has some interesting implications---importantly, the adversary can store these messages along with their MAC and replay them later.}
As far as the adversary's power goes: we, as usual in cryptography,
restrict the adversary to be efficient (i.e., to run in polynomial time).
But in the MAC security game we also allow the adversary to obtain
MAC tags on messages of its choice.
This captures the reality that in many systems, an adversary can trick
an honest system into MACing adversarial messages.
For example, if an email-backup system MACs every email that a
user receives, an adversary may be able to obtain MAC tags on messages
of its choice by sending emails to the backup system.


\subsection{MACs require pseudorandomness}
The fact that it is even possible to construct a MAC seems a bit surprising---in effect, for a MAC to satisfy the definition, the tag has to effectively be random. But the only \textquote{randomness} that we have is the key $k$---to generate tags for arbitrarily many messages, we need much more randomness than one key's worth. This seems impossible.
How can we generate a large number of random-looking tags from only a single
short random key?

We get ourselves out of this conundrum by observing that the adversary
must be an \emph{efficient} algorithm.
So while we cannot generate a large number of truly random bits from a
short key, we can---under appropriate and reasonable
cryptographic assumptions---generate a large number of bits that \emph{look}
truly random from the perspective of any efficient algorithm.
We call these bits \emph{pseudorandom}.

This surprising and powerful idea leads us to our next cryptographic primitive\ldots

\section{Pseudorandom Functions}

A pseudorandom function is defined over a keyspace $\calK$,
and input spacei $\calX$ and output space $\calY$.
To be useful a pseudorandom function must satisfy the following
security definition:

\begin{definition}[Pseudorandom Function, PRF]\label{defn:prf}
A function $F: \mathcal{K} \cross \calX \rightarrow \calY$ is a pseudorandom
function if all efficient algorithms $\mathcal{A}$ win
the following game with probability $\tfrac{1}{2} + \text{\textquote{negligible}}$: 

  \begin{itemize}[noitemsep]
    \item The challenger samples a random bit $b \gets \zo$ and a key $k \getsr \calK$.
    \item If $b = 0$, the challenger sets $f(\cdot) \deq F(k, \cdot)$.
    \item If $b = 1$, the challenger sets $f(\cdot) \getsr \Funs[\calX, \calY]$.
      \marginnote{Here, $\Funs$ is the set of all functions from $\calX$ to $\calY$.}
    \item Then for $i = 1, 2, \dots$ (polynomially many times):
      \begin{itemize}
        \item The adversary sends the challenger a values $x_i \in \calX$.
        \item The challenger responds with $y_i \gets f(x_i) \in \calY$.
      \end{itemize}
    \item The adversary outputs a guess $\hat{b}$ at the bit $b$.
    \item The adversary wins if $b = \hat{b}$.
  \end{itemize}
First, the challenger will sample a random $b \leftarrow \bin$ and a key $k \leftarrow \mathcal{K}$. 
\end{definition}

The adversary can trivially win this game with
probability $\tfrac{1}{2}$ by just guessing the
bit $b$ at random.
This definition asserts that
no efficient adversary can do much better than that.

If we have such a pseudorandom function $F$, we
could easily construct a MAC---we can just use the
message as the input to the pseudorandom function
along with the key: $\MAC(k, m) \deq F(k, m)$.


\subsection{Constructing pseudorandom functions from one-wayness}

It is not at all obvious that pseudorandom functions should exist
at all! They seem like a very magical primitive indeed.

One surprising fact is that if there exists \emph{any} function that
is ``hard to invert,`` in a sense we will define, then pseudorandom
functions exist.
For example, if you believe that factoring large numbers is difficult
(as many people do), then pseudorandom functions exist.

In particular the following definition captures the notion of a
function that is hard to invert:
\begin{definition}[One-Way Function]\label{def:owf}
A function $f \colon \calX \to \calY$ is a \emph{one-way function} if
for all efficient adversaries $\calA$, 
\[ \Pr[f(\calA(f(x))) = f(x) \colon x \getsr \calX ] \leq \text{``negligible''}.\]
\end{definition}

\todo{Cite theorem}
Having defined one-way functions, we now have the following surprising and
non-obvious result:\marginnote{Notice that if
$\classP = \classNP$, one-way functions do not exist,
and therefore psuedorandom functions do not exist.
}
\begin{theorem}
Psueodorandom functions exist if and only if one-way functions exist.
\end{theorem}

In practice, we assume that:
\begin{itemize}
  \item the function $f(x) \deq \text{SHA256}(x)$ is a one-way function
    where the domain is the set of 256-bit strings,
  \item the function $f(x) \deq \text{AES}(x, 0^{128})$ is a one-way function,
    where the domain is the set of 128-bit strings, and
  \item the function $f(x) \deq 2^x \bmod p$ is a one-way function
    on domain $\{1, \dots, p\}$, for a sufficiently large prime $p$.
\end{itemize}

\subsection{Pseudorandom functions in practice}

In practice, we use the Advanced Encryption
Standard (AES) as a pseudorandom function.
The AES function on key length $\kappa \in \{128, 192, 256\}$
has the type signature
$\mathsf{AES}: \bin^\kappa \times \bin^{128} \rightarrow \bin^{128}$.
That is, it takes a 128-bit input and generates a 128-bit output.
\marginnote{We
don't have any mathematical proof that AES is
a pseudorandom function. However, 
it has undergone a tremendous amount of cryptanalysis and the best attacks on AES are only marginally better than the obvious brute-force attacks.}

\section{From pseudorandom functions to MACs}

\paragraph{MACs for short messages.}
Using AES as a pseudorandom function on a 128-bit domain,
we can build a MAC for 128-bit messages as described above
: $\MAC(k, m) \deq \mathsf{AES}_k(m)$. 
However, since AES takes only 128 bits as input, using AES
directly, we can only authenticate 128-bit messages. 

\paragraph{Insecure ways to construct a MAC for long messages.}
A bad way to construct a MAC for long messages from a pseudorandom 
function $F$ for 128-bit messages is
just to chop our message $m$ up into 128-bit blocks
$m = (m_1, m_2, \dots)$
and MAC each block separately.
Our tag, then, would look something like $\left(F(k,m_1), F(k,m_1)\right)$.
However, there is a problem! Given the tag $t = (t_1, t_2)$ for a message $m=(m_1, m_2)$, we can easily generate a valid tag 
$t' = (t_2, t_1)$ for a different message $m'=(m_2, m_1)$. 

\paragraph{MACs for long messages: The easy way.}
\marginnote{
Notice that we cannot use AES as the pseudorandom
function $F$ in this construction, since AES only
takes a 128-bit input.
In this case, we would need a collision-resistant
hash function $H \colon \zo^* \to \zo^{128}$,
but it is always possible to find collisions
in hash functions with 128-bit output in time $2^{64}$.
So such a MAC can never be secure against attackers running
in time $2^{64}$.
}
If we have a pseudorandom function $F$ with an input space of 256-bits,
we can construct a MAC on arbitrary-length messages using the ``hash-and-sign'' paradigm.
In particular, we use a collision-resistant hash function $H\colon \zo^* \to \zo^{256}$ 
(\cref{def:crhf}) and we define the MAC on message space $\zo^*$ as:
\[ \MAC(k,m) \deq F(k, H(m)).\]


In practice, we typically do not construct MACs in this way because
collision-resistant hash functions are typically more expensive to
compute (per bit of input) than pseudorandom functions, such as AES.

\subsection{MACs for long messages: Cipher-Block Chaining MAC}

\marginnote{
Applying the PRF to the last block using an independent random key is important.
If we do not use a new key, an adversary can mount a length-extension attack.
That is, if the adversary asks for $t = \mathsf{MAC}(k, m_1)$ and $t' = \mathsf{MAC}(k, t)$, $t'$ is also a valid key for the original message with two zero blocks attached $\mathsf{MAC}(k, m_1 \| 0 \| 0)$. The chain of AES applications becomes equivalent, since zero blocks are equivalent to skipping the XOR and adding AES applications.
}

A common and secure way to construct a MAC for long messages from a MAC for short messages
is to \emph{chain} the output of each of these calls to the pseudorandom function.
Given our chopped message $(m_1, m_2, \ldots, m_n)$, we will generate $t_1 = F(k,m_1)$ as before. When generating $t_2$, we will first XOR $t_1$ into the input: 
$t_2 = F(k, m_2 \oplus t_1)$. This continues until the end of the message, at which point have a tag $t_n$.
Finally, we apply the PRF \emph{with a different key} $k'$ to the value $t_n$ and output this tag $t \gets F(k', t_n)$. 
This construction is called CBC-MAC or CMAC. % TOOD: explanation of why we need a second key here would be useful---the diagram in lecture 9/21 was very useful to understand the same-key attack.


\begin{figure}[htpb]
	\centering
	\begin{tikzpicture}
		\node (m0) [draw] {$m_1$};
    \node[below =1cm of m0] (f0) [draw] {$F(k, \cdot)$};
		\node[below =0.5cm of f0] (t0) [draw] {$t_1$};
		\draw [->] (m0) edge (f0) (f0) edge (t0);

		\node[right=0.75cm of m0] (m1) [draw] {$m_2$};
		\node[below =0.25cm of m1] (m1xt0) {$\oplus$};
		\draw [->] (t0) - ++(0.75,0) -- ++(0.75,1.83) -- (m1xt0);
    \node[below =1cm of m1] (f1) [draw] {$F(k, \cdot)$};
		\node[below =0.5cm of f1] (t1) [draw] {$t_2$};
		\draw [->] (m1) edge (m1xt0) (m1xt0) edge (f1) (f1) edge (t1);

		\node[right=0.75cm of m1] (m2) [draw] {$m_3$};
		\node[below =0.25cm of m2] (m2xt1) {$\oplus$};
		\draw [->] (t1) - ++(0.75,0) -- ++(0.75,1.83) -- (m2xt1);
    \node[below =1cm of m2] (f2) [draw] {$F(k, \cdot)$};
		\node[below =0.5cm of f2] (t2) [draw] {$t_3$};
    \node[right=0.5cm of t2] (out) [draw] {$F(k', \cdot)$};
    \node[right=0.5cm of out] (outp) [draw] {$t$};
		\draw [->] (m2) edge (f2) (f2) edge (t2);
    \draw [->] (t2) edge (out);
    \draw [->] (out) edge (outp);
	\end{tikzpicture}
  \caption{The CBC-MAC construction.}
	\label{fig:}
\end{figure}

CBC-MAC is going out of favor for two reasons:
\begin{enumerate}
  \item It is impossible to parallelize the MAC computation:
        the chaining procedure is inherently sequential so you 
        cannot speed it up, even if you have a computer with many 
        CPU cores.
  \item Computing the MAC requires one PRF invocation \emph{per block} of the message.
        There are even faster MACs that require only one PRF invocation
        \emph{per message} total, 
        plus a number of fast ``non-cryptographic'' operations
        per message block.
        These MACs can be faster than CBC-MAC on some processors.
        The GMAC construction we will see next is one example.
\end{enumerate}

% TODO: diagram

\subsection{A parallelizable MAC: Carter-Wegman MAC}

We now describe a different way to authenticate long messages.
This MAC scheme is parallelizable and also requires only one 
single PRF invocation per message authenticated (independent of
the message length).
The construction is named the Carter-Wegman MAC, after its
inventors.\cite{CW81}
Modern encryption schemes, including AES-GCM (\cref{sec:enc:gcm})
use a Carter-Wegman-style MAC as a key ingredient.

For this construction, we will use the notation
$\Z_p$ to indicate the set of integers modulo $p$
with addition and multiplication modulo $p$.
So $x + y \in \Z_p$ means that we add $x$ and $y$
as integers and reduce the result modulo~$p$.
Typically, we will think of $p$ as a prime---of
64 bits, for example.

The MAC uses a fixed a prime number $p$ as a parameter,
where $p \approx 2^n$ on security parameter $n$.\marginnote{So
in practice, we take $p \approx 2^{128}$
for 128-bit security.}
The MAC uses a pseudorandom function 
$F \colon \calK \times \Z_p \to \zon$.
\marginnote{Here, the input space of the pseudorandom 
function $F$ is the set of integers in $\{0, \dots, p-1\}$.
Given a pseudorandom function on bitstrings, it is indeed
possible to construct one that operates on numbers in 
$\Z_p$ like this by interpreting each number as a bitstring.}

The keyspace for the MAC is $\calK_\mathsf{MAC} = \Z_p \times \calK$;
so the MAC key consists of a $\Z_p$ element and a key for 
the pseudorandom function.
The message space for the MAC is $\calM = \Z_p^{\leq L}$,
the set of vectors of integers of $\Z_p$ elements of length
at most $L$.
Here, assume that the message vector has length at least $1$.

The MAC construction is then:

\medskip
\noindent
$\MAC(k, m \in \Z^{\leq L}_p) \to t$.
\begin{itemize}
  \item Parse the key $(r, k_F) \gets k \in \Z_p \times \calK$. 
  \item Parse the message into chunks as $(m_0, \dots, m_{\ell-1}) \gets m \in \Z_p^\ell$.
  \item Compute $R \gets m_0 + m_1 r + m_2 r^2 + \cdots + m_{\ell-1} r^{\ell-1} \in \Z_p$. \\
        (Essentially we are viewing the blocks of the message $m$ as coefficients of a degree-$(t-1)$
        polynomial. We then evaluate this polynomial at the secret point $r$ determined by the MAC key.)
  \item Output $t \gets F(k_F, R) \in \Z_p$ as the MAC tag.
\end{itemize}

\paragraph{Security intuition.}
The security argument here goes in two steps:
\begin{itemize}
  \item First, we appeal to the PRF security property to argue
        that we can replace the values $F(k_F, R)$ used to generate
        the tags with truly random values.
        Now, the adversary gets no information 

  \item Now, as long as the adversary does not find two messages
        $m,m' \in \calM$ with $m \neq m'$, the tags that the 
        adversary sees are just uniform random values, so the
        tags give the adversary no information on the secret point $r$.

        Finally, we use a fact about polynomials about $\Z_p$ to show
        that the probability that the adversary ever finds a pair of
        unequal messages $m$ and $m'$ is small.
        In particular, if we evaluate two distinct non-zero polynomials
        of degree $(t-1)$ at a random point in $\Z_p$, the probability
        that they are equal is 
        
\end{itemize}
